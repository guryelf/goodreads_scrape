#!/usr/bin/env python3
"""
Goodreads Scraper -     url = input("ğŸ”— Different list URL? (Enter = Best Books Ever): ").strip()
    if not url:
        url = "https://www.goodreads.com/list/show/1.Best_Books_Ever"
    
    delay = input("â±ï¸ Delay between requests (seconds, default: 1.5): ").strip()
    if not delay:
        delay = "1.5"
    
    output = input("ğŸ’¾ Output filename (default: goodreads_books.csv): ").strip()
    if not output:
        output = "goodreads_books.csv" Script
This script checks required libraries and runs the scraper
"""

import subprocess
import sys
import os
from pathlib import Path

def check_requirements():
    """Check if required libraries are installed"""
    required_packages = [
        'requests', 'beautifulsoup4', 'pandas', 'lxml', 'numpy', 'tqdm'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            missing_packages.append(package)
    
    return missing_packages

def install_requirements():
    """Install missing libraries"""
    print("ğŸ”§ Installing required libraries...")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])
        print("âœ… Libraries successfully installed!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Library installation error: {e}")
        return False

def run_scraper():
    """Run the scraper script with user-defined parameters"""
    print("\nğŸ“š Starting Goodreads Scraper")
    
    # KullanÄ±cÄ±dan parametreler al
    pages = input("How many pages do you want to be processed?(Default 10):  ").strip()
    if not pages:
        pages = "10"
    
    url = input("ğŸ”— Different List URL? (Enter = Best Books Ever): ").strip()
    if not url:
        url = "https://www.goodreads.com/list/show/1.Best_Books_Ever"
    
    delay = input("â±ï¸  Dely between requests(Default:1.5 secs): ").strip()
    if not delay:
        delay = "1.5"
    
    output = input("ï¿½ Output file name(Default: goodreads_books.csv): ").strip()
    if not output:
        output = "goodreads_books.csv"
    
    print(f"\nğŸ¯ Target: {url}")
    print(f"ğŸ“„ {pages} pages (approximately {int(pages) * 100} books) will be processed...")
    print(f"â±ï¸ This process may take {int(pages) * 0.5}-{int(pages)} minutes...\n")
    
    try:
        # Change to src folder
        original_dir = os.getcwd()
        os.chdir('src')
        
        # Run scraper with arguments
        command = [
            sys.executable, "goodreads_scraper.py",
            "--pages", pages,
            "--url", url,
            "--delay", delay,
            "--output", output
        ]
        
        result = subprocess.run(command, capture_output=True, text=True)
        
        # Return to original folder
        os.chdir(original_dir)
        
        if result.returncode == 0:
            print("âœ… Scraping completed successfully!")
            print(f"ğŸ“Š Output: {result.stdout}")
            
            # Check if data file was created
            data_file = Path(f'data/{output}')
            if data_file.exists():
                print(f"\nğŸ“ Data file created: {data_file}")
                print(f"ğŸ“ File size: {data_file.stat().st_size / 1024:.1f} KB")
            
        else:
            print("âŒ Error occurred during scraping:")
            print(result.stderr)
            
    except Exception as e:
        print(f"âŒ Script execution error: {e}")
        os.chdir(original_dir)

def show_project_info():
    """Display project information"""
    print("="*60)
    print("ğŸ“š GOODREADS WEB SCRAPER PROJECT")
    print("="*60)
    print("ğŸ¯ Purpose: Collect most popular books from Goodreads")
    print("ğŸ“Š Target: ~1000 book records")
    print("ğŸ”§ Technology: Python + BeautifulSoup + Pandas")
    print("ğŸ“ Folder structure:")
    print("   â”œâ”€â”€ src/                 # Python code")
    print("   â”œâ”€â”€ data/                # Collected data")
    print("   â”œâ”€â”€ requirements.txt     # Required libraries")
    print("   â””â”€â”€ README.md           # Documentation")
    print("="*60)

def main():
    """Main function"""
    show_project_info()
    
    # Check required libraries
    missing = check_requirements()
    
    if missing:
        print(f"\nâš ï¸ Missing libraries detected: {', '.join(missing)}")
        
        install_choice = input("\nğŸ¤” Would you like to install missing libraries? (y/n): ").lower().strip()
        
        if install_choice in ['y', 'yes']:
            if not install_requirements():
                print("âŒ Installation failed. Run 'pip install -r requirements.txt' manually.")
                return
        else:
            print("âŒ Scraper cannot run without required libraries.")
            print("ğŸ’¡ Install command: pip install -r requirements.txt")
            return
    
    else:
        print("\nâœ… All required libraries are installed!")
    
    # Run scraper
    run_choice = input("\nğŸš€ Would you like to start the scraper? (y/n): ").lower().strip()
    
    if run_choice in ['y', 'yes']:
        run_scraper()
        
        # Analysis suggestion
        print("\nğŸ’¡ TIP: To analyze the collected data:")
        print("   cd src && python analyze_data.py")
        
    else:
        print("ğŸ‘‹ Have a great day! Run again when you're ready.")

if __name__ == "__main__":
    main()